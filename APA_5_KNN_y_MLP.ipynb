{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to upgrade packages\n",
    "# !pip install pandas --upgrade --user --quiet\n",
    "# !pip install numpy --upgrade --user --quiet\n",
    "# !pip install scipy --upgrade --user --quiet\n",
    "# !pip install statsmodels --upgrade --user --quiet\n",
    "# !pip install scikit-learn --upgrade --user --quiet\n",
    "# !pip install graphviz --upgrade --user --quiet\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "pd.set_option('precision', 3)\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra imports\n",
    "from sklearn.metrics import confusion_matrix,\\\n",
    "                classification_report, accuracy_score\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from graphviz import Digraph\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy.random import normal\n",
    "from numpy.random import uniform\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.metrics import precision_score, classification_report, recall_score\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(true, pred, classes):\n",
    "    \"\"\"\n",
    "    Function for pretty printing confusion matrices\n",
    "    \"\"\"\n",
    "    cm =pd.DataFrame(confusion_matrix(true, pred), index=classes,\n",
    "                 columns=classes)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    return cm\n",
    "\n",
    "def graphMLP(vars,layers,intercepts):\n",
    "    \"\"\"\n",
    "    Function for plotting the weights of a mlp\n",
    "    \"\"\"\n",
    "    f = Digraph('')\n",
    "    f.attr(rankdir='LR')\n",
    "    for i,l in enumerate(layers):\n",
    "        if i==0:\n",
    "            for j in range(l.shape[1]):\n",
    "                for k, v in enumerate(vars):\n",
    "                    f.edge(v, 'L%dN%d'%(i,j), label=str(l[k,j]))\n",
    "            f.node('ILI', shape='doublecircle')                    \n",
    "            for k in range(intercepts[i].shape[0]): \n",
    "                f.edge('ILI', \n",
    "                       'L%dN%d'%(i,k), \n",
    "                       label=str(intercepts[i][k]))\n",
    "        else:\n",
    "            for j in range(l.shape[1]):\n",
    "                for k in range(layers[i-1].shape[1]):\n",
    "                    f.edge('L%dN%d'%(i-1,k), \n",
    "                           'L%dN%d'%(i,j), \n",
    "                           label=str(l[k,j]))\n",
    "            f.node('L%dI'%(i-1), shape='doublecircle')                  \n",
    "            for k in range(intercepts[i].shape[0]):\n",
    "                f.edge('L%dI'%(i-1), \n",
    "                       'L%dN%d'%(i,k), \n",
    "                       label=str(intercepts[i][k]))    \n",
    "                \n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4567)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Admissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the admissions dataset for our examples on this lab. \n",
    "\n",
    "This dataset contains the next variables: \n",
    "* GRE (Graduate Record Exam scores)\n",
    "* GPA (Grade Point Average) and\n",
    "* rank (prestige of the undergraduate institution)\n",
    "\n",
    "This variables should affect admission into a graduate school.\n",
    "\n",
    "\n",
    " The target variable, admit/don't admit, is a binary variable, which we want to characterize\n",
    "and, if possible, to predict (a model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Admis = read_csv(\"Admissions.csv\", delimiter=',')\n",
    "Admis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will treat all the variables gre and gpa as continuous.\n",
    "\n",
    "The variable rank takes on the values 1 through 4, so we can fairly treat it as numerical (although, in rigour, it is ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Admis.describe()\n",
    "\n",
    "N = Admis.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split the available data into learning and test sets, selecting randomly 2/3 and 1/3 of the data We do this for a honest estimation of prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling protocol\n",
    "\n",
    "This time we will use a Train and Test partitions and we will use the cross-validation score to compare the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(63)\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "        train_test_split(Admis[['gre', 'gpa', 'rank']], Admis.admit, test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "We will use a simple pre-processing. Just scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_test = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(index=['Logistic Regression'],columns=['Accuracy (cv)', 'Mean recall (cv)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Logistic Regression\n",
    "\n",
    "We are going to use the logistic regression as a baseline, and we will try to improve the results given by the baseline with the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "scores = cross_val_score(logreg, X_train, y_train, cv=5)\n",
    "y_pred_lr = logreg.predict(X_train)\n",
    "\n",
    "acc = np.mean(scores)\n",
    "acc\n",
    "confusion(y_train,y_pred_lr, ['noadmit','admit'])\n",
    "recalls = np.mean(cross_val_score(logreg, X_train, y_train, cv=5,scoring='recall'))\n",
    "\n",
    "results_df.loc['Logistic Regression', :] = [acc, recalls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just with the baseline we are already obtaining a 69% of accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg.coef_\n",
    "logreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "This model compares the sample to classify to the samples in the train set using a metric. This model has a very fast training phase, but might have a slower inference depending on the metric used or training set size. \n",
    "\n",
    "One remark of this model is that you can even implement your own metric adapted to your data. This can be super useful when you are working with strange shaped data. \n",
    "\n",
    "sklearn has already implemented the next metrics:\n",
    "\n",
    "| identifier    | class name          | args    | distance function            |\n",
    "|---------------|---------------------|---------|------------------------------|\n",
    "| “euclidean”   | EuclideanDistance   |         | sqrt(sum((x - y)^2))         |\n",
    "| “manhattan”   | ManhattanDistance   |         | sum(\\|x - y\\|)               |\n",
    "| “chebyshev”   | ChebyshevDistance   |         | max(\\|x - y\\|)               |\n",
    "| “minkowski”   | MinkowskiDistance   | p       | sum(\\|x - y\\|^p)^(1/p)       |\n",
    "| “wminkowski”  | WMinkowskiDistance  | p, w    | sum(\\|w * (x - y)\\|^p)^(1/p) |\n",
    "| “seuclidean”  | SEuclideanDistance  | V       | sqrt(sum((x - y)^2 / V))     |\n",
    "| “mahalanobis” | MahalanobisDistance | V or VI | sqrt((x - y)' V^-1 (x - y))  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myknn = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "myknn.fit(X_train, y_train)\n",
    "scores = cross_val_score(myknn, X_train, y_train, cv=5)\n",
    "scores_recall = cross_val_score(myknn, X_train, y_train, cv=5,scoring='recall')\n",
    "\n",
    "y_pred = myknn.predict(X_train)\n",
    "\n",
    "confusion(y_train,y_pred, ['noadmit','admit'])\n",
    "\n",
    "acc=np.mean(scores)\n",
    "recalls = np.mean(scores_recall)\n",
    "\n",
    "results_df.loc['KNN', :] = [acc, recalls]\n",
    "\n",
    "print('Accuracy:{}\\nRecalls:{}'.format(acc,recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the accuracy the logistic regression looks like a better model, but is missclasifying a lot of the samples of the minoritary class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check what would happen if we had a model which predicted only the majoritary class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train == 0).sum()/len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would obtain a 68% of accuracy with a totally useless model.  How can we compare the two models with that much imbalance? \n",
    "\n",
    "In this case we will also use the mean recall. i.e. the mean between the ratio between real admited and total number of admited and the ratio between real not admited and total not admited.  \n",
    "\n",
    "If we calculate it at hand it would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "33/(33+52) # True admited / Real number of admited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use sklearn implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall of LR \n",
    "recall_score(y_train, y_pred_lr,average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall of KNN\n",
    "recall_score(y_train, y_pred,average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this measure you can obtain the \n",
    "* macro average: The mean of the recalls for each class.\n",
    "* micro average: The global metric without taking into account the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_train,y_pred, ['noadmit','admit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train, y_pred,average='macro'), np.mean(recall_score(y_train, y_pred,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train, y_pred,average='micro'), (33+167)/(33+52+16+167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have another metric we can use to compare how good is our model. If the model classified all the samples as not admited it would have a recall of 0, which would discurage us to chose it as our model.\n",
    "\n",
    "There are other metrics we can use to evaluate how well is working our model. We will talk more about them on further labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred,target_names=['noadmit','admit']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets really compare our two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have that even though knn has less accuracy it has better recall than the LR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layered Perceptron (Yay! Neural Networks!)\n",
    "\n",
    "MLP is almost the simplest of the Neural Network models. This model is based on combining linearly the input of the previous layer and applying an activation function to the result over different layers. \n",
    "\n",
    "When it has more than a hidden layer it is called a __Deep__ Neural Network. This hidden layers give the model complexity and expresivity, but also add more weights that you will need to train.\n",
    "\n",
    "The most important parameters of the MLP are: \n",
    "* Architecture (number of layers and number of neurons by layer). \n",
    "* Activation function. \n",
    "* alpha (regularization term).\n",
    "* Solver ‘lbfgs’, ‘sgd’, ‘adam’. \n",
    "\n",
    "As you can see, this model has a lot of hyperparameters.\n",
    "\n",
    "We are going to start with a 1 neuron dummy MLP with logistic activation function and no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nnet = MLPClassifier(hidden_layer_sizes=[1],\n",
    "                           alpha=0,\n",
    "                           activation='logistic',\n",
    "                           max_iter=200,\n",
    "                           solver='lbfgs',random_state=42)\n",
    "model_nnet.fit(X_train,y_train);\n",
    "\n",
    "scores = cross_val_score(model_nnet, X_train, y_train, cv=5)\n",
    "scores_recall = cross_val_score(model_nnet, X_train, y_train, cv=5,scoring='recall')\n",
    "\n",
    "y_pred = model_nnet.predict(X_train)\n",
    "\n",
    "\n",
    "confusion(y_train,y_pred, ['noadmit','admit'])\n",
    "\n",
    "acc=np.mean(scores)\n",
    "recalls = np.mean(scores_recall)\n",
    "print('Accuracy:{}\\nRecalls:{}'.format(acc,recalls))\n",
    "results_df.loc['MLP[1]', :] = [acc, recalls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this dummy mlp is already winning our other models in Admit recall. Let's see how this is looking on the inside.\n",
    "\n",
    "Here we can see this mlp weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nnet.coefs_\n",
    "model_nnet.intercepts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at it as a graph. Isn't this structure familiar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphMLP(Admis.columns[1:], model_nnet.coefs_, model_nnet.intercepts_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this graph the gre, gpa, rank and ILI represent the input layer,which would be our training samples. \n",
    "L0N0 is our hidden neuron. L01 the extra intercept and L1N0 our output neuron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we are doing, lets make a bigger network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nnet = MLPClassifier(hidden_layer_sizes=[2,2],\n",
    "                           alpha=0,\n",
    "                           activation='logistic',\n",
    "                           max_iter=200,\n",
    "                           solver='lbfgs',random_state=42)\n",
    "model_nnet.fit(X_train,y_train);\n",
    "\n",
    "scores = cross_val_score(model_nnet, X_train, y_train, cv=5)\n",
    "scores_recall = cross_val_score(model_nnet, X_train, y_train, cv=5,scoring='recall')\n",
    "\n",
    "y_pred = model_nnet.predict(X_train)\n",
    "\n",
    "\n",
    "confusion(y_train,y_pred, ['noadmit','admit'])\n",
    "\n",
    "acc=np.mean(scores)\n",
    "recalls = np.mean(scores_recall)\n",
    "print('Accuracy:{}\\nRecalls:{}'.format(acc,recalls))\n",
    "results_df.loc['MLP[2,2]', :] = [acc, recalls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a winner!\n",
    "\n",
    "Also, our model complexity is increasing, which might increase the training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graphMLP(Admis.columns[1:], model_nnet.coefs_, model_nnet.intercepts_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some weights are large (two orders of magnitude larger then others) This is no good, since it makes the model unstable (i.e., small changes in some inputs may entail significant changes in the network, because of the large weights)\n",
    "\n",
    "One way to avoid this is by regularizing the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nnet = MLPClassifier(hidden_layer_sizes=[2,2],\n",
    "                           alpha=0.001,\n",
    "                           activation='logistic',\n",
    "                           max_iter=200,\n",
    "                           solver='lbfgs',random_state=42)\n",
    "model_nnet.fit(X_train,y_train);\n",
    "\n",
    "scores = cross_val_score(model_nnet, X_train, y_train, cv=5)\n",
    "scores_recall = cross_val_score(model_nnet, X_train, y_train, cv=5,scoring='recall')\n",
    "\n",
    "y_pred = model_nnet.predict(X_train)\n",
    "\n",
    "\n",
    "confusion(y_train,y_pred, ['noadmit','admit'])\n",
    "\n",
    "acc=np.mean(scores)\n",
    "recalls = np.mean(scores_recall)\n",
    "print('Accuracy:{}\\nRecalls:{}'.format(acc,recalls))\n",
    "results_df.loc['MLP[2,2]-alpha', :] = [acc, recalls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the master of the models. And also with small weights i.e. robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graphMLP(Admis.columns[1:], model_nnet.coefs_, model_nnet.intercepts_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how can we know that this is the best possible model? Whi alpha is 0.00001 and not 25? Is it better to increase depht or increase neurons on this case?\n",
    "\n",
    "MLP results depend *Strongly* of its hyperparameter configuration. Now we are going to see different strategies to obtain the best hyperparameters for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn has specific functions for parameter search so we can tune the parameters of a model.\n",
    "\n",
    " We are going to use a grid search that will use a cross validation strategy to evaluate the results for each combination of parameters. At the end the best model will be returned\n",
    "\n",
    " In order to find the best network architecture, we are going to explore two methods:\n",
    "\n",
    "1. Explore different numbers of hidden units in one hidden layer, with no regularization\n",
    "2. Fix a large number of hidden units in one hidden layer, and explore different regularization values (recommended)\n",
    "\n",
    "doing both (explore different numbers of hidden units AND regularization values) is usually a waste of computing \n",
    "resources (but notice that it would admit it)\n",
    "\n",
    "Let's start with 1.\n",
    "\n",
    "set desired sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [2*i for i in range(1,11)]\n",
    "sizes = sizes + [[2*i,2*i] for i in range(1,5)]\n",
    "\n",
    "len(sizes), sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2,[2,2]] - > 2 modelos -> entrenamos, calculamos las métricas de cv -> comparamos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time=time()\n",
    "model_nnet = MLPClassifier(alpha=0,\n",
    "                           activation='logistic',\n",
    "                           max_iter=500,\n",
    "                           solver='lbfgs',\n",
    "                           random_state=42)\n",
    "\n",
    "trc = GridSearchCV(estimator=model_nnet,\n",
    "                   scoring=['accuracy', 'recall'],\n",
    "                   param_grid={'hidden_layer_sizes': sizes},\n",
    "                   cv=10,\n",
    "                   return_train_score=True,\n",
    "                   refit='recall')\n",
    "\n",
    "model_10CV = trc.fit(X_train, y_train)\n",
    "print(timedelta(seconds=(time()-init_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10CV.best_params_ , model_10CV.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the results of the cv to decide which is the best parameter configuracion.\n",
    "\n",
    "According to the criteria of having the greatest recall, we would choose the layer configuration [4]. But if we were usign the accuracy we would chose [2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_10CV.cv_results_).loc[:,['param_hidden_layer_sizes', 'mean_test_accuracy','std_test_accuracy',\n",
    "                                        'mean_test_recall','std_test_recall']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the decays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decays = [10**i for i in np.arange(-5,0,0.1)]\n",
    "print(decays[:10]) \n",
    "\n",
    "len(decays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "model_nnet = MLPClassifier(activation='logistic',\n",
    "                           hidden_layer_sizes=[4],\n",
    "                           max_iter=500,\n",
    "                           solver='lbfgs',\n",
    "                           random_state=42)\n",
    "\n",
    "trc = GridSearchCV(estimator=model_nnet,\n",
    "                   scoring=['accuracy', 'recall'],\n",
    "                   param_grid={'alpha': decays},\n",
    "                   cv=10,\n",
    "                   return_train_score=True,\n",
    "                   refit='recall')\n",
    "\n",
    "model_10CV = trc.fit(X_train, y_train)\n",
    "print(timedelta(seconds=(time() - init_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model_10CV.cv_results_).loc[:,['param_alpha', 'mean_test_accuracy','std_test_accuracy',\n",
    "                                        'mean_test_recall','std_test_recall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10CV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10CV.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So,according to our experiments the best parameter configuration would be: \n",
    "* Architecture [4]\n",
    "* Alpha 1.9952623149688746e-05\n",
    "\n",
    "Lets calculate our cross-validation scores of the best model found trained (model_10CV.best_estimator_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model_10CV.best_estimator_, X_train, y_train, cv=5)\n",
    "scores_recall = cross_val_score(model_10CV.best_estimator_, X_train, y_train, cv=5,scoring='recall')\n",
    "\n",
    "y_pred = model_10CV.predict(X_train)\n",
    "confusion(y_train,y_pred, ['noadmit','admit'])\n",
    "\n",
    "acc=np.mean(scores)\n",
    "recalls = np.mean(scores_recall)\n",
    "print('Accuracy:{}\\nRecalls:{}'.format(acc,recalls))\n",
    "results_df.loc['MLP[4]-reg', :] = [acc, recalls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see our final results on the test set of the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_10CV.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "\n",
    "confusion(y_test,y_pred, ['noadmit','admit'])\n",
    "recalls = recall_score(y_test,y_pred,average=None)\n",
    "print('Accuracy:{}\\nRecalls:{}'.format(acc,recalls))\n",
    "\n",
    "recall_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we obtain our test metrics, which tell us that we did a good job selecting our model, as it is generalizing quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeled Faces in the Wild\n",
    "\n",
    "Now we are going to try the same with a more complex dataset. \n",
    "\n",
    "This dataset contains images in black and white of public personalities. The task is to clasify the images with the proper name. \n",
    "\n",
    "Disclaimer: Even though this is an image dataset we will try it as a full numerical data. It is complex enough for showing MLP with real data and *someone* likes image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: {}\".format(n_samples))\n",
    "print(\"n_features: {}\".format(n_features))\n",
    "print(\"n_classes: {}\".format(n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling an pre-processing\n",
    "\n",
    "We will use the same resampling and pre-processing that on the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_test = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this dataset is quite big and it has more features than samples. That might be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can see that this dataset is quite unbalanced. We could get almost a 40% of accuracy only with a constant model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.countplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train==3).sum()/len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now we have too much classes to just look at ones recall. \n",
    "We will handle this problem by taking into account the mean recacall, as well as the accuracy.\n",
    "\n",
    "Also, we will check how much *Time* is taking our models to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(index=['KNN'],columns=['Accuracy', 'Recall (mean)', 'Time(s)'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: KNN\n",
    "\n",
    "We will use knn as a baseline, because is a very fast model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "myknn = KNeighborsClassifier(n_neighbors=5)\n",
    "myknn.fit(X_train, y_train)\n",
    "\n",
    "training_time = time()-init_time\n",
    "print(timedelta(seconds=training_time))\n",
    "\n",
    "scores = cross_val_score(myknn, X_train, y_train, cv=5)\n",
    "scores_recall = cross_val_score(myknn, X_train, y_train, cv=5,scoring='recall_macro')\n",
    "\n",
    "y_pred = myknn.predict(X_train)\n",
    "\n",
    "acc=np.mean(scores)\n",
    "recalls = np.mean(scores_recall)\n",
    "\n",
    "\n",
    "confusion(y_train,y_pred,target_names )\n",
    "print('Accuracy:{}\\nRecalls:{}'.format(acc,recalls))\n",
    "results_df.loc['KNN',:] = [acc,recalls,training_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain quite good results with this model. Lets see with the MLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "model_nnet = MLPClassifier(hidden_layer_sizes=[5],\n",
    "                           alpha=0.01,\n",
    "                           activation='logistic',\n",
    "                           max_iter=200,\n",
    "                           solver='lbfgs',random_state=42)\n",
    "model_nnet.fit(X_train,y_train);\n",
    "\n",
    "training_time = time()-init_time\n",
    "print(timedelta(seconds=training_time))\n",
    "\n",
    "scores = cross_val_score(model_nnet, X_train, y_train, cv=5)\n",
    "scores_recall = cross_val_score(model_nnet, X_train, y_train, cv=5,scoring='recall_macro')\n",
    "\n",
    "y_pred = model_nnet.predict(X_train)\n",
    "\n",
    "acc=np.mean(scores)\n",
    "recalls = np.mean(scores_recall)\n",
    "\n",
    "\n",
    "confusion(y_train,y_pred,target_names )\n",
    "print('Accuracy:{}\\nRecalls:{}'.format(acc,recalls))\n",
    "results_df.loc['MLP[5]-0.01',:] = [acc,recalls,training_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives better results but it is several times slower than the knn. \n",
    "\n",
    "If we wanted to try different hyperparameters we would need a lot of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try another strategy instead. We can use PCA to reduce the dimensionality of the data. This way the training of the model would be faster, and might reduce the noise in the data.\n",
    "\n",
    "\n",
    "First lets see how many components do we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca = PCA().fit(X_train)\n",
    "\n",
    "n_components =(pca.explained_variance_ratio_.cumsum() < 0.99).sum()\n",
    "n_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just 339 of the variables, we could maintain 99 % of the variance of the data. Thas a huge reduction from the original 1850."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components).fit(X_train)\n",
    "\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "Lets see how affects our smaller data to the knn results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "myknn = KNeighborsClassifier(n_neighbors=5)\n",
    "myknn.fit(X_train_pca, y_train)\n",
    "\n",
    "training_time = time()-init_time\n",
    "print(timedelta(seconds=training_time))\n",
    "\n",
    "scores = cross_val_score(myknn, X_train_pca, y_train, cv=5,scoring='accuracy')\n",
    "scores_recall = cross_val_score(myknn, X_train_pca, y_train, cv=5,scoring='recall_macro')\n",
    "\n",
    "y_pred = myknn.predict(X_train_pca)\n",
    "\n",
    "acc=np.mean(scores)\n",
    "recalls = np.mean(scores_recall)\n",
    "\n",
    "\n",
    "confusion(y_train,y_pred,target_names )\n",
    "print('Accuracy:{}\\nRecalls:{}'.format(acc,recalls))\n",
    "results_df.loc['KNN-PCA',:] = [acc,recalls,training_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are obtaining the same results with a fraction of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "model_nnet = MLPClassifier(hidden_layer_sizes=[5],\n",
    "                           alpha=0.01,\n",
    "                           activation='logistic',\n",
    "                           max_iter=200,\n",
    "                           solver='lbfgs',random_state=42)\n",
    "model_nnet.fit(X_train_pca,y_train);\n",
    "\n",
    "training_time = time()-init_time\n",
    "print(timedelta(seconds=training_time))\n",
    "\n",
    "scores = cross_val_score(model_nnet, X_train_pca, y_train, cv=5)\n",
    "scores_recall = cross_val_score(model_nnet, X_train_pca, y_train, cv=5,scoring='recall_macro')\n",
    "\n",
    "y_pred = model_nnet.predict(X_train_pca)\n",
    "\n",
    "acc=np.mean(scores)\n",
    "recalls = np.mean(scores_recall)\n",
    "\n",
    "\n",
    "confusion(y_train,y_pred,target_names )\n",
    "print('Accuracy:{}\\nRecalls:{}'.format(acc,recalls))\n",
    "results_df.loc['MLP[5]-0.01-PCA',:] = [acc,recalls,training_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP has lost a bit on both metrics but is faster, now we can try to improve its results using cross-validation.\n",
    "\n",
    "We will use the same technique than before, but now we are testing fewer parameters, as the model takes longer to train. We will also change the number of CV partitions into 5, to go faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [2*i for i in range(1,4)]\n",
    "sizes = sizes + [[2*i,2*i] for i in range(1,4)]\n",
    "sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do 5-cv with 6 layer configuration and we spend 1.5 seconds with each training, we *should* spend approximately 45 seconds with the CV process. If all the architectures took the same time to train. \n",
    "\n",
    "If we had done it without the pca we would take more than two minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "\n",
    "model_nnet = MLPClassifier(alpha=0,\n",
    "                           activation='logistic',\n",
    "                           hidden_layer_sizes=2,\n",
    "                           max_iter=500,\n",
    "                           solver='lbfgs',random_state=42)\n",
    "\n",
    "trc = GridSearchCV(estimator=model_nnet, \n",
    "                   param_grid ={'hidden_layer_sizes':sizes},\n",
    "                   scoring=['accuracy', 'recall_macro'],\n",
    "                   cv=5,\n",
    "                   return_train_score=True,\n",
    "                  refit='recall_macro')\n",
    "model_5CV = trc.fit(X_train_pca,y_train)\n",
    "model_5CV.best_score_\n",
    "model_5CV.best_params_\n",
    "print(timedelta(seconds=(time()-init_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took almost 1:21 minutes. \n",
    "\n",
    "The best architecture found is [6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_5CV.cv_results_).loc[:,['mean_fit_time','param_hidden_layer_sizes', 'mean_test_accuracy','std_test_accuracy',\n",
    "                                        'mean_test_recall_macro','std_test_recall_macro']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each training took more time than the one we used to estimate. This might be because the number of neurons of the architectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see the decays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decays = [0, 0.1]\n",
    "decays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "\n",
    "model_nnet = MLPClassifier(alpha=0,\n",
    "                           activation='logistic',\n",
    "                           hidden_layer_sizes=[6],\n",
    "                           max_iter=500,\n",
    "                           solver='lbfgs',random_state=42)\n",
    "\n",
    "trc = GridSearchCV(estimator=model_nnet, \n",
    "                   param_grid ={'alpha':decays},\n",
    "                   scoring=['accuracy', 'recall_macro'],\n",
    "                   cv=5,\n",
    "                   return_train_score=True,\n",
    "                  refit='recall_macro')\n",
    "model_5CV = trc.fit(X_train_pca,y_train)\n",
    "model_5CV.best_score_\n",
    "model_5CV.best_params_\n",
    "print(timedelta(seconds=(time()-init_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_5CV.cv_results_).loc[:,['mean_fit_time','param_alpha', 'mean_test_accuracy','std_test_accuracy',\n",
    "                                        'mean_test_recall_macro','std_test_recall_macro']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have found our best MLP (among these configurations). Lets check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "model_nnet = MLPClassifier(hidden_layer_sizes=[6],\n",
    "                           alpha=0.1,\n",
    "                           activation='logistic',\n",
    "                           max_iter=200,\n",
    "                           solver='lbfgs',random_state=42)\n",
    "model_nnet.fit(X_train_pca,y_train);\n",
    "\n",
    "training_time = time()-init_time\n",
    "print(timedelta(seconds=training_time))\n",
    "\n",
    "scores = cross_val_score(model_nnet, X_train_pca, y_train, cv=5)\n",
    "scores_recall = cross_val_score(model_nnet, X_train_pca, y_train, cv=5,scoring='recall_macro')\n",
    "\n",
    "y_pred = model_nnet.predict(X_train_pca)\n",
    "\n",
    "acc=np.mean(scores)\n",
    "recalls = np.mean(scores_recall)\n",
    "\n",
    "\n",
    "confusion(y_train,y_pred,target_names )\n",
    "print('Accuracy:{}\\nRecalls:{}'.format(acc,recalls))\n",
    "results_df.loc['MLP[6]-0.1-PCA',:] = [acc,recalls,training_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have choosen our best model. We can check if it generalizes using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_nnet.predict(X_test_pca)\n",
    "\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "\n",
    "confusion(y_test,y_pred, target_names)\n",
    "recalls = recall_score(y_test,y_pred,average=None)\n",
    "print('Accuracy:{}\\nRecalls:\\n{}'.format(acc,recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained quite good results, and the model generalizes so it also predicts acurately on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
